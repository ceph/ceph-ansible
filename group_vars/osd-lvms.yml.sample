---
# Variables here are applicable to all host groups NOT roles

# This sample file generated by generate_group_vars_sample.sh

# Dummy variable to avoid error because ansible does not recognize the
# file as a good configuration file when no variable in it.
dummy:

# You can override default vars defined in defaults/main.yml here,
# but I would advice to use host or group vars instead


###########
# GENERAL #
###########

# Even though OSD nodes should not have the admin key
# at their disposal, some people might want to have it
# distributed on OSD nodes. Setting 'copy_admin_key' to 'true'
# will copy the admin key to the /etc/ceph/ directory
#copy_admin_key: false


##############
# CEPH OPTIONS
##############

# Devices to be used as OSDs
# You can pre-provision disks that are not present yet.
# Ansible will just skip them. Newly added disk will be
# automatically configured during the next run.
#


# Declare devices to be used as OSDs
# All scenario(except 3rd) inherit from the following device declaration
# Note: This scenario uses the ceph-disk tool to provision OSDs

#devices:
#  - /dev/sdb
#  - /dev/sdc
#  - /dev/sdd
#  - /dev/sde

#devices: []


#'osd_auto_discovery'  mode prevents you from filling out the 'devices' variable above.
# Device discovery is based on the Ansible fact 'ansible_devices'
# which reports all the devices on a system. If chosen all the disks
# found will be passed to ceph-disk. You should not be worried on using
# this option since ceph-disk has a built-in check which looks for empty devices.
# Thus devices with existing partition tables will not be used.
#
#osd_auto_discovery: false

# Encrypt your OSD device using dmcrypt
# If set to True, no matter which osd_objecstore and osd_scenario you use the data will be encrypted
#dmcrypt: False

#osd_scenario: dummy
#valid_osd_scenarios:
#  - lvm



# More device granularity for Bluestore
#
# ONLY if osd_objectstore: bluestore is enabled.
#
# By default, if 'bluestore_wal_devices' is empty, it will get the content of 'dedicated_devices'.
# If set, then you will have a dedicated partition on a specific device for block.wal.
#
# Example of what you will get:
# [root@ceph-osd0 ~]# blkid /dev/sd*
# /dev/sda: PTTYPE="gpt"
# /dev/sda1: UUID="39241ae9-d119-4335-96b3-0898da8f45ce" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="961e7313-bdb7-49e7-9ae7-077d65c4c669"
# /dev/sda2: PARTLABEL="ceph block" PARTUUID="bff8e54e-b780-4ece-aa16-3b2f2b8eb699"
# /dev/sdb: PTTYPE="gpt"
# /dev/sdb1: PARTLABEL="ceph block.db" PARTUUID="0734f6b6-cc94-49e9-93de-ba7e1d5b79e3"
# /dev/sdc: PTTYPE="gpt"
# /dev/sdc1: PARTLABEL="ceph block.wal" PARTUUID="824b84ba-6777-4272-bbbd-bfe2a25cecf3"
# Note: This option uses the ceph-disk tool
#bluestore_wal_devices: "{{ dedicated_devices }}"

# III. Use ceph-volume to create OSDs from logical volumes.
# Use 'osd_scenario: lvm' to enable this scenario.
# when using lvm, not collocated journals.
# lvm_volumes is a list of dictionaries.
#
# Filestore: Each dictionary must contain a data, journal and vg_name key. Any
# logical volume or logical group used must be a name and not a path.  data
# can be a logical volume, device or partition. journal can be either a lv or partition.
# You can not use the same journal for many data lvs.
# data_vg must be the volume group name of the data lv, only applicable when data is an lv.
# journal_vg is optional and must be the volume group name of the journal lv, if applicable.
# For example:
# lvm_volumes:
#   - data: data-lv1
#     data_vg: vg1
#     journal: journal-lv1
#     journal_vg: vg2
#     crush_device_class: foo
#   - data: data-lv2
#     journal: /dev/sda1
#     data_vg: vg1
#   - data: data-lv3
#     journal: /dev/sdb1
#     data_vg: vg2
#   - data: /dev/sda
#     journal: /dev/sdb1
#   - data: /dev/sda1
#     journal: /dev/sdb1
#
# Bluestore: Each dictionary must contain at least data. When defining wal or
# db, it must have both the lv name and vg group (db and wal are not required).
# This allows for four combinations: just data, data and wal, data and wal and
# db, data and db.
# For example:
# lvm_volumes:
#   - data: data-lv1
#     data_vg: vg1
#     wal: wal-lv1
#     wal_vg: vg1
#     crush_device_class: foo
#   - data: data-lv2
#     db: db-lv2
#     db_vg: vg2
#   - data: data-lv3
#     wal: wal-lv1
#     wal_vg: vg3
#     db: db-lv3
#     db_vg: vg3
#   - data: data-lv4
#     data_vg: vg4
#   - data: /dev/sda
#   - data: /dev/sdb1

#lvm_volumes: []
#crush_device_class: ""
#osds_per_device: 1
