---
## SCENARIO 3: N JOURNAL DEVICES FOR N OSDS

- include: ../check_devices.yml

# NOTE (leseb): the prepare process must be parallelized somehow...
# if you have 64 disks with 4TB each, this will take a while
# since Ansible will sequential process the loop

- set_fact:
    ceph_disk_cli_options: "--cluster {{ cluster }} --bluestore"
  when:
    - osd_objectstore == 'bluestore'
    - ceph_release_num.{{ ceph_release }} >= ceph_release_num.luminous

- set_fact:
    ceph_disk_cli_options: "--cluster {{ cluster }} --filestore"
  when:
    - osd_objectstore == 'filestore'
    - ceph_release_num.{{ ceph_release }} >= ceph_release_num.luminous

- set_fact:
    ceph_disk_cli_options: "--cluster {{ cluster }}"
  when:
    - osd_objectstore == 'filestore'
    - ceph_release_num.{{ ceph_release }} < ceph_release_num.luminous

- name: prepare filestore osd disk(s) with a dedicated journal device
  command: "ceph-disk prepare {{ ceph_disk_cli_options }} {{ item.1 }} {{ item.2 }}"
  with_together:
    - "{{ legacy_devices }}"
    - "{{ legacy_raw_journal_devices }}"
  changed_when: false
  when:
    - item.0.get("skipped") or item.0.get("rc", 0) != 0
    - osd_objectstore == 'filestore'
    - not osd_auto_discovery

- name: manually prepare bluestore osd disk(s) with a dedicated device for db and wal
  command: "ceph-disk prepare {{ ceph_disk_cli_options }} --block.db {{ item.1 }} --block.wal {{ item.2 }} {{ item.3 }}"
  with_together:
    - "{{ parted_results.results }}"
    - "{{ raw_journal_devices }}"
    - "{{ bluestore_wal_devices }}"
    - "{{ devices }}"
  when:
    - not item.0.get("skipped")
    - item.0.get("rc", 0) != 0
    - osd_objectstore == 'bluestore'
    - not osd_auto_discovery

- include: ../activate_osds.yml
