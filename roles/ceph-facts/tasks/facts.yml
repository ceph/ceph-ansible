---
- name: check if it is atomic host
  stat:
    path: /run/ostree-booted
  register: stat_ostree

- name: set_fact is_atomic
  set_fact:
    is_atomic: "{{ stat_ostree.stat.exists }}"

- name: import_tasks container_binary.yml
  import_tasks: container_binary.yml

- name: set_fact is_podman
  set_fact:
    is_podman: "{{ podman_binary.stat.exists }}"

# In case ansible_python_interpreter is set by the user,
# ansible will not discover python and discovered_interpreter_python
# will not be set
- name: set_fact discovered_interpreter_python
  set_fact:
    discovered_interpreter_python: "{{ ansible_python_interpreter }}"
  when: ansible_python_interpreter is defined

# Set ceph_release to ceph_stable by default
- name: set_fact ceph_release ceph_stable_release
  set_fact:
    ceph_release: "{{ ceph_stable_release }}"

- name: set_fact monitor_name ansible_hostname
  set_fact:
    monitor_name: "{{ hostvars[item]['ansible_hostname'] }}"
  delegate_to: "{{ item }}"
  delegate_facts: true
  with_items: "{{ groups.get(mon_group_name, []) }}"
  run_once: true
  when:
    - groups.get(mon_group_name, []) | length > 0
    - not mon_use_fqdn | bool

- name: set_fact monitor_name ansible_fqdn
  set_fact:
    monitor_name: "{{ hostvars[item]['ansible_fqdn'] }}"
  delegate_to: "{{ item }}"
  delegate_facts: true
  with_items: "{{ groups.get(mon_group_name, []) }}"
  run_once: true
  when:
    - groups.get(mon_group_name, []) | length > 0
    - mon_use_fqdn | bool

- name: find a running monitor
  when: groups.get(mon_group_name, []) | length > 0
  block:
    - name: set_fact container_exec_cmd
      set_fact:
        container_exec_cmd: "{{ container_binary }} exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] if not rolling_update else hostvars[mon_host | default(groups[mon_group_name][0])]['ansible_hostname'] }}"
      when:
        - containerized_deployment | bool

    - name: find a running mon container
      command: "{{ container_binary }} ps -q --filter name=ceph-mon-{{ hostvars[item]['ansible_hostname'] }}"
      register: find_running_mon_container
      failed_when: false
      run_once: true
      delegate_to: "{{ item }}"
      with_items: "{{ groups.get(mon_group_name, []) }}"
      when:
        - containerized_deployment | bool

    - name: check for a ceph mon socket
      shell: stat --printf=%n {{ rbd_client_admin_socket_path }}/{{ cluster }}-mon*.asok
      changed_when: false
      failed_when: false
      check_mode: no
      register: mon_socket_stat
      run_once: true
      delegate_to: "{{ item }}"
      with_items: "{{ groups.get(mon_group_name, []) }}"
      when:
        - not containerized_deployment | bool

    - name: check if the ceph mon socket is in-use
      command: grep -q {{ item.stdout }} /proc/net/unix
      changed_when: false
      failed_when: false
      check_mode: no
      register: mon_socket
      run_once: true
      with_items: "{{ mon_socket_stat.results }}"
      when:
        - not containerized_deployment | bool
        - item.rc == 0

    - name: set_fact running_mon - non_container
      set_fact:
        running_mon: "{{ hostvars[item.item.item]['inventory_hostname'] }}"
      with_items: "{{ mon_socket.results }}"
      run_once: true
      when:
        - not containerized_deployment | bool
        - item.rc is defined
        - item.rc == 0

    - name: set_fact running_mon - container
      set_fact:
        running_mon: "{{ item.item }}"
      run_once: true
      with_items: "{{ find_running_mon_container.results }}"
      when:
        - containerized_deployment | bool
        - item.stdout_lines | default([]) | length > 0

    - name: set_fact _container_exec_cmd
      set_fact:
        _container_exec_cmd: "{{ container_binary }} exec ceph-mon-{{ hostvars[groups[mon_group_name][0] if running_mon is undefined else running_mon]['ansible_hostname'] }}"
      when:
        - containerized_deployment | bool

    # this task shouldn't run in a rolling_update situation
    # because it blindly picks a mon, which may be down because
    # of the rolling update
    - name: is ceph running already?
      command: "{{ timeout_command }} {{ _container_exec_cmd | default('') }} ceph --cluster {{ cluster }} -s -f json"
      changed_when: false
      failed_when: false
      check_mode: no
      register: ceph_current_status
      run_once: true
      delegate_to: "{{ groups[mon_group_name][0] if running_mon is undefined else running_mon }}"
      when:
        - not rolling_update | bool

# set this as a default when performing a rolling_update
# so the rest of the tasks here will succeed
- name: set_fact ceph_current_status rc 1
  set_fact:
    ceph_current_status:
      rc: 1
  when: rolling_update or groups.get(mon_group_name, []) | length == 0

- name: create a local fetch directory if it does not exist
  file:
    path: "{{ fetch_directory }}"
    state: directory
  delegate_to: localhost
  changed_when: false
  become: false
  when: cephx | bool or generate_fsid | bool

- name: get current fsid
  command: "{{ timeout_command }} {{ container_exec_cmd }} ceph --admin-daemon /var/run/ceph/{{ cluster }}-mon.{{ hostvars[mon_host | default(groups[mon_group_name][0])]['ansible_hostname'] }}.asok config get fsid"
  register: rolling_update_fsid
  delegate_to: "{{ mon_host | default(groups[mon_group_name][0]) }}"
  until: rolling_update_fsid is succeeded
  when: rolling_update | bool

- name: set_fact fsid
  set_fact:
    fsid: "{{ (rolling_update_fsid.stdout | from_json).fsid }}"
  when: rolling_update | bool

- name: set_fact ceph_current_status (convert to json)
  set_fact:
    ceph_current_status: "{{ ceph_current_status.stdout | from_json }}"
  when:
    - not rolling_update | bool
    - ceph_current_status.rc == 0

- name: set_fact fsid from ceph_current_status
  set_fact:
    fsid: "{{ ceph_current_status.fsid }}"
  when: ceph_current_status.fsid is defined

- name: fsid related tasks
  when:
    - generate_fsid | bool
    - ceph_current_status.fsid is undefined
    - not rolling_update | bool
  block:
  - name: generate cluster fsid
    command: "{{ hostvars[groups[mon_group_name][0]]['discovered_interpreter_python'] }} -c 'import uuid; print(str(uuid.uuid4()))'"
    register: cluster_uuid
    delegate_to: "{{ groups[mon_group_name][0] }}"
    run_once: true

  - name: set_fact fsid
    set_fact:
      fsid: "{{ cluster_uuid.stdout }}"

- name: set_fact mds_name ansible_hostname
  set_fact:
    mds_name: "{{ ansible_hostname }}"
  when: not mds_use_fqdn | bool

- name: set_fact mds_name ansible_fqdn
  set_fact:
    mds_name: "{{ ansible_fqdn }}"
  when: mds_use_fqdn | bool

- name: set_fact rbd_client_directory_owner ceph
  set_fact:
    rbd_client_directory_owner: ceph
  when: rbd_client_directory_owner is not defined
      or not rbd_client_directory_owner

- name: set_fact rbd_client_directory_group rbd_client_directory_group
  set_fact:
    rbd_client_directory_group: ceph
  when: rbd_client_directory_group is not defined
      or not rbd_client_directory_group

- name: set_fact rbd_client_directory_mode 0770
  set_fact:
    rbd_client_directory_mode: "0770"
  when: rbd_client_directory_mode is not defined
      or not rbd_client_directory_mode

- name: resolve device link(s)
  command: readlink -f {{ item }}
  changed_when: false
  check_mode: no
  with_items: "{{ devices }}"
  register: devices_prepare_canonicalize
  when:
    - devices is defined
    - inventory_hostname in groups.get(osd_group_name, [])
    - not osd_auto_discovery | default(False) | bool

- name: set_fact build devices from resolved symlinks
  set_fact:
    devices: "{{ devices | default([]) + [ item.stdout ] }}"
  with_items: "{{ devices_prepare_canonicalize.results }}"
  when:
    - devices is defined
    - inventory_hostname in groups.get(osd_group_name, [])
    - not osd_auto_discovery | default(False) | bool

- name: set_fact build final devices list
  set_fact:
    devices: "{{ devices | reject('search','/dev/disk') | list | unique }}"
  when:
    - devices is defined
    - inventory_hostname in groups.get(osd_group_name, [])
    - not osd_auto_discovery | default(False) | bool

- name: set_fact devices generate device list when osd_auto_discovery
  set_fact:
    devices: "{{ (devices | default([]) + [ item.key | regex_replace('^', '/dev/') ]) | unique }}"
  with_dict: "{{ ansible_devices }}"
  when:
    - osd_auto_discovery | default(False) | bool
    - ansible_devices is defined
    - item.value.removable == "0"
    - item.value.sectors != "0"
    - item.value.partitions|count == 0
    - item.value.holders|count == 0
    - item.key is not match osd_auto_discovery_exclude

- name: set_fact ceph_uid for debian based system - non container
  set_fact:
    ceph_uid: 64045
  when:
    - not containerized_deployment | bool
    - ansible_os_family == 'Debian'

- name: set_fact ceph_uid for red hat or suse based system - non container
  set_fact:
    ceph_uid: 167
  when:
    - not containerized_deployment | bool
    - ansible_os_family in ['RedHat', 'Suse']

- name: set_fact ceph_uid for debian based system - container
  set_fact:
    ceph_uid: 64045
  when:
    - containerized_deployment | bool
    - ceph_docker_image_tag | string is search("ubuntu")

- name: set_fact ceph_uid for red hat based system - container
  set_fact:
    ceph_uid: 167
  when:
    - containerized_deployment | bool
    - (ceph_docker_image_tag | string is search("latest")
      or ceph_docker_image_tag | string is search("centos")
      or ceph_docker_image_tag | string is search("fedora")
      or ceph_docker_image_tag | string is search("rhceph")
      or (ansible_distribution == 'RedHat'))

- name: set_fact rgw_hostname
  set_fact:
    rgw_hostname: "{% set _value = ansible_hostname -%}
    {% for key in (ceph_current_status['servicemap']['services']['rgw']['daemons'] | list) -%}
    {% if key == ansible_fqdn -%}
    {% set _value = key -%}
    {% endif -%}
    {% endfor -%}
    {{ _value }}"
  when:
    - inventory_hostname in groups.get(rgw_group_name, []) or inventory_hostname in groups.get(nfs_group_name, [])
    - ceph_current_status['servicemap'] is defined
    - ceph_current_status['servicemap']['services'] is defined
    - ceph_current_status['servicemap']['services']['rgw'] is defined

- name: set_fact osd_pool_default_pg_num
  set_fact:
    osd_pool_default_pg_num: "{{ ceph_conf_overrides.get('global', {}).get('osd_pool_default_pg_num', ceph_osd_pool_default_pg_num) }}"

- name: set_fact osd_pool_default_size
  set_fact:
    osd_pool_default_size: "{{ ceph_conf_overrides.get('global', {}).get('osd_pool_default_size', ceph_osd_pool_default_size) }}"

- name: set_fact osd_pool_default_min_size
  set_fact:
    osd_pool_default_min_size: "{{ ceph_conf_overrides.get('global', {}).get('osd_pool_default_min_size', ceph_osd_pool_default_min_size) }}"

- name: check if the ceph conf exists
  stat:
    path: '/etc/ceph/{{ cluster }}.conf'
  register: ceph_conf

- name: get default crush rule value from ceph configuration
  command: grep 'osd pool default crush rule' /etc/ceph/{{ cluster }}.conf
  register: crush_rule_variable
  changed_when: false
  check_mode: no
  failed_when: false
  when: ceph_conf.stat.exists

- name: set_fact osd_pool_default_crush_rule
  set_fact:
    osd_pool_default_crush_rule: "{{ crush_rule_variable.stdout.split(' = ')[1] if crush_rule_variable.get('rc', 1) | int == 0 else ceph_osd_pool_default_crush_rule }}"

- name: import_tasks set_monitor_address.yml
  import_tasks: set_monitor_address.yml
  when: groups.get(mon_group_name, []) | length > 0

- name: import_tasks set_radosgw_address.yml
  import_tasks: set_radosgw_address.yml
  when: inventory_hostname in groups.get(rgw_group_name, [])

- name: set ntp service name depending on OS family
  block:
  - name: set ntp service name for Debian family
    set_fact:
      ntp_service_name: ntp
    when: ansible_os_family == 'Debian'

  - name: set ntp service name for Red Hat family
    set_fact:
      ntp_service_name: ntpd
    when: ansible_os_family in ['RedHat', 'Suse']

- name: set chrony daemon name RedHat and Ubuntu based OSs
  block:
    - name: set chronyd daemon name for RedHat based OSs
      set_fact:
        chrony_daemon_name: chronyd
      when: ansible_os_family in ["RedHat", "Suse"]

    - name: set chronyd daemon name for Ubuntu based OSs
      set_fact:
        chrony_daemon_name: chrony
      when: ansible_os_family == "Debian"

- name: set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli
  set_fact:
    use_new_ceph_iscsi: "{{ (gateway_ip_list  == '0.0.0.0' and gateway_iqn | length == 0 and client_connections | length == 0 and rbd_devices | length == 0) | bool | ternary(true, false) }}"

- name: set_fact ceph_run_cmd
  set_fact:
    ceph_run_cmd: "{{ container_binary + ' run --rm --net=host -v /etc/ceph:/etc/ceph:z -v /var/lib/ceph/:/var/lib/ceph/:z -v /var/log/ceph/:/var/log/ceph/:z --entrypoint=ceph ' + ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else 'ceph' }}"
  delegate_to: "{{ item }}"
  delegate_facts: True
  run_once: True
  with_items:
    - "{{ groups[mon_group_name] if groups[mon_group_name] | default([]) | length > 0 else [] }}"
    - "{{ groups[mds_group_name] if groups[mds_group_name] | default([]) | length > 0 else [] }}"
    - "{{ groups[client_group_name] if groups[client_group_name] | default([]) | length > 0 else [] }}"

- name: set_fact ceph_admin_command
  set_fact:
    ceph_admin_command: "{{ hostvars[item]['ceph_run_cmd'] }} -n client.admin -k /etc/ceph/{{ cluster }}.client.admin.keyring"
  delegate_to: "{{ item }}"
  delegate_facts: True
  run_once: True
  with_items:
    - "{{ groups[mon_group_name] if groups[mon_group_name] | default([]) | length > 0 else [] }}"
    - "{{ groups[mds_group_name] if groups[mds_group_name] | default([]) | length > 0 else [] }}"
    - "{{ groups[client_group_name] if groups[client_group_name] | default([]) | length > 0 else [] }}"
