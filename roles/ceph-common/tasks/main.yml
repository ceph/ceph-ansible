---
- name: check if it is Atomic host
  stat: path=/run/ostree-booted
  register: stat_ostree
  always_run: true

- name: set fact for using Atomic host
  set_fact:
    is_atomic: '{{ stat_ostree.stat.exists }}'

- include: ./checks/check_system.yml

- include: ./checks/check_mandatory_vars.yml

# Set ceph_release
- include: ./release.yml

- include: ./checks/check_firewall.yml
  when: check_firewall
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./misc/system_tuning.yml
  when: osd_group_name in group_names
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./pre_requisites/prerequisite_rh_storage_iso_install.yml
  when:
    - ceph_rhcs
    - ceph_rhcs_iso_install
  tags:
    - package-install
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./pre_requisites/prerequisite_rh_storage_cdn_install.yml
  when:
    - ceph_rhcs
    - ceph_rhcs_cdn_install
    - ansible_os_family == "RedHat"
  tags:
    - package-install
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./installs/install_on_redhat.yml
  when:
    ansible_os_family == 'RedHat' and
    not ceph_rhcs_iso_install and
    not is_atomic
  tags:
    - package-install
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./installs/install_rh_storage_on_redhat.yml
  when:
    - ansible_distribution == "RedHat"
    - ceph_rhcs
  tags:
    - package-install
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./installs/install_on_debian.yml
  when:
    - ansible_os_family == 'Debian'
    - not ceph_rhcs
  tags:
    - package-install
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./installs/install_rh_storage_on_debian.yml
  when:
    - ansible_os_family == 'Debian'
    - ceph_rhcs
  tags:
    - package-install
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./misc/ntp_redhat.yml
  when:
    - ansible_os_family == 'RedHat'
    - ntp_service_enabled
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: ./misc/ntp_debian.yml
  when:
    - ansible_os_family == 'Debian'
    - ntp_service_enabled
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False

- include: facts.yml

- set_fact:
    dir_owner: ceph
    dir_group: ceph
    dir_mode: "0755"
  when: (ceph_release_num.{{ ceph_release }} > ceph_release_num.hammer) and not is_atomic

- set_fact:
    dir_owner: root
    dir_group: root
    dir_mode: "0755"
  when: (ceph_release_num.{{ ceph_release }} < ceph_release_num.infernalis) and not is_atomic

- set_fact:
    key_owner: root
    key_group: root
    key_mode: "0600"
  when: (ceph_release_num.{{ ceph_release }} < ceph_release_num.infernalis) and not is_atomic

- set_fact:
    key_owner: ceph
    key_group: ceph
    key_mode: "0600"
  when: (ceph_release_num.{{ ceph_release }} > ceph_release_num.hammer) and not is_atomic

- set_fact:
    activate_file_owner: ceph
    activate_file_group: ceph
    activate_file_mode: "0644"
  when: (ceph_release_num.{{ ceph_release }} > ceph_release_num.hammer) and not is_atomic

- set_fact:
    activate_file_owner: root
    activate_file_group: root
    activate_file_mode: "0644"
  when: (ceph_release_num.{{ ceph_release }} < ceph_release_num.infernalis) and not is_atomic

- set_fact:
    key_owner: root
    key_group: root
    key_mode: "0600"
    dir_owner: root
    dir_group: root
    dir_mode: "0755"
    rbd_client_directory_owner: root
    rbd_client_directory_group: root
    activate_file_mode: "0644"
    rbd_client_directory_mode: "0770"
  when: is_atomic

- set_fact:
    rbd_client_directory_owner: root
  when:
    - ceph_release_num.{{ ceph_release }} < ceph_release_num.infernalis
    - rbd_client_directory_owner is not defined
      or not rbd_client_directory_owner

- set_fact:
    rbd_client_directory_owner: ceph
  when:
    - ceph_release_num.{{ ceph_release }} > ceph_release_num.hammer
    - rbd_client_directory_owner is not defined
      or not rbd_client_directory_owner

- set_fact:
    rbd_client_directory_group: root
  when:
    - ceph_release_num.{{ ceph_release }} < ceph_release_num.infernalis
    - rbd_client_directory_group is not defined
      or not rbd_client_directory_group

- set_fact:
    rbd_client_directory_group: ceph
  when:
    - ceph_release_num.{{ ceph_release }} > ceph_release_num.hammer
    - rbd_client_directory_group is not defined
      or not rbd_client_directory_group

- set_fact:
    rbd_client_directory_mode: "1777"
  when:
    - ceph_release_num.{{ ceph_release }} < ceph_release_num.infernalis
    - rbd_client_directory_mode is not defined
      or not rbd_client_directory_mode

- set_fact:
    rbd_client_directory_mode: "0770"
  when:
    - ceph_release_num.{{ ceph_release }} > ceph_release_num.hammer
    - rbd_client_directory_mode is not defined
      or not rbd_client_directory_mode

- name: check for a ceph socket
  shell: "stat /var/run/ceph/*.asok > /dev/null 2>&1"
  changed_when: false
  failed_when: false
  always_run: true
  register: socket

- name: check for a rados gateway socket
  shell: "stat {{ rbd_client_admin_socket_path }}*.asok > /dev/null 2>&1"
  changed_when: false
  failed_when: false
  always_run: true
  register: socketrgw

- name: create a local fetch directory if it does not exist
  local_action: file path={{ fetch_directory }} state=directory
  changed_when: false
  become: false
  run_once: true
  when: cephx or generate_fsid

- name: generate cluster fsid
  local_action: shell python -c 'import uuid; print(str(uuid.uuid4()))' | tee {{ fetch_directory }}/ceph_cluster_uuid.conf
    creates="{{ fetch_directory }}/ceph_cluster_uuid.conf"
  register: cluster_uuid
  become: false
  when:
    - generate_fsid
    - ceph_current_fsid.rc != 0

- name: reuse cluster fsid when cluster is already running
  local_action: shell echo {{ fsid }} | tee {{ fetch_directory }}/ceph_cluster_uuid.conf
    creates="{{ fetch_directory }}/ceph_cluster_uuid.conf"
  become: false
  when: ceph_current_fsid.rc == 0

- name: read cluster fsid if it already exists
  local_action: command cat {{ fetch_directory }}/ceph_cluster_uuid.conf
    removes="{{ fetch_directory }}/ceph_cluster_uuid.conf"
  changed_when: false
  register: cluster_uuid
  become: false
  always_run: true
  when: generate_fsid

- name: create ceph conf directory
  file:
    path: /etc/ceph
    state: directory
    owner: "{{ dir_owner }}"
    group: "{{ dir_group }}"
    mode: "{{ dir_mode }}"
  when: not is_atomic

- name: generate ceph configuration file
  action: config_template
  args:
    src: ceph.conf.j2
    dest: /etc/ceph/{{ cluster }}.conf
    owner: "{{ dir_owner }}"
    group: "{{ dir_group }}"
    mode: "{{ activate_file_mode }}"
    config_overrides: "{{ ceph_conf_overrides }}"
    config_type: ini
  notify:
    - restart ceph mons
    - restart ceph mons on ubuntu
    - restart ceph mons with systemd
    - restart ceph osds
    - restart ceph osds on ubuntu
    - restart ceph osds with systemd
    - restart ceph mdss
    - restart ceph mdss on ubuntu
    - restart ceph mdss with systemd
    - restart ceph rgws
    - restart ceph rgws on ubuntu
    - restart ceph rgws on red hat
    - restart ceph rgws with systemd
    - restart ceph nfss

- name: create rbd client directory
  file:
    path: "{{ item }}"
    state: directory
    owner: "{{ rbd_client_directory_owner }}"
    group: "{{ rbd_client_directory_group }}"
    mode: "{{ rbd_client_directory_mode }}"
  with_items:
    - "{{ rbd_client_admin_socket_path }}"
    - "{{ rbd_client_log_path }}"
  when: rbd_client_directories

- name: configure cluster name
  lineinfile:
    dest: /etc/sysconfig/ceph
    insertafter: EOF
    create: yes
    line: "CLUSTER={{ cluster }}"
  when: ansible_os_family == "RedHat"

# NOTE(leseb): we are performing the following check
# to ensure any Jewel installation will not fail.
# The following commit https://github.com/ceph/ceph/commit/791eba81a5467dd5de4f1680ed0deb647eb3fb8b
# fixed a package issue where the path was the wrong.
# This bug is not yet on all the distros package so we are working around it
# Impacted versions:
# - Jewel from UCA: https://bugs.launchpad.net/ubuntu/+source/ceph/+bug/1582773
# - Jewel from latest Canonical 16.04 distro
# - All previous versions from Canonical
# - Infernalis from ceph.com
- name: check /etc/default/ceph exist
  stat:
    path: /etc/default/ceph
  register: etc_default_ceph
  always_run: true
  when: ansible_os_family == "Debian"

- name: configure cluster name
  lineinfile:
    dest: /etc/default/ceph
    insertafter: EOF
    create: yes
    line: "CLUSTER={{ cluster }}"
  when:
    - ansible_os_family == "Debian"
    - etc_default_ceph.stat.exists
    - not etc_default_ceph.stat.isdir

- name: configure cluster name
  lineinfile:
    dest: /etc/default/ceph/ceph
    insertafter: EOF
    create: yes
    line: "CLUSTER={{ cluster }}"
  when:
    - ansible_os_family == "Debian"
    - etc_default_ceph.stat.exists
    - etc_default_ceph.stat.isdir
