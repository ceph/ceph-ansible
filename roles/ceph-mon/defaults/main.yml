---
# You can override vars by using host or group vars

###########
# GENERAL #
###########
mon_group_name: mons

# ACTIVATE BOTH FSID AND MONITOR_SECRET VARIABLES FOR NON-VAGRANT DEPLOYMENT
monitor_secret: "{{ monitor_keyring.stdout }}"
admin_secret: 'admin_secret'
mgr_secret: 'mgr_secret'

# Secure your cluster
# This will set the following flags on all the pools:
# * nosizechange
# * nopgchange
# * nodelete

secure_cluster: false
secure_cluster_flags:
  - nopgchange
  - nodelete
  - nosizechange


###############
# CRUSH RULES #
###############
crush_rule_config: false

# Definition for EC profile (arguments to ceph osd erasure-code-profile set)
ec42_profile:
  name: ec_42
  plugin: jerasure
  ec_config:
    k: 4
    m: 2
    crush-device-class: "hdd"

# Definition for replicated rule (arguments to ceph osd crush rule create-replicated)
osd_rep_rule:
  name: osd_replicated
  root: default
  type: host
  class: hdd
  default: false

# Definition for erasure-coded rule (arguments to ceph osd crush rule create-erasure) 
ec42_rule:
  name: ec42
  ec_profile: ec_42
  default: false

# Definition for simple rule (arguments to ceph osd crush rule create-simple)
crush_rule_hdd:
  name: HDD
  root: HDD
  type: host
  default: false

# Definition for simple rule (arguments to ceph osd crush rule create-simple)
crush_rule_ssd:
  name: SSD
  root: SSD
  type: host
  default: false

# Rules with arguments for create-simple
crush_rules:
  - "{{ crush_rule_hdd }}"
  - "{{ crush_rule_ssd }}"

# Rules with arguments for create-replicated
crush_rep_rules:
  - "{{ osd_rep_rule }}"

# EC Profiles to create
ec_profiles:
  - "{{ ec42_profile }}"

# Rules with arguments for create-erasure-coded
crush_ec_rules:
  - "{{ ec42_rule }}"

# Caution: this will create crush roots and racks according to hostvars {{ osd_crush_location }}
# and will move hosts into them which might lead to significant data movement in the cluster!
#
# In order for the playbook to create CRUSH hierarchy, you have to setup your Ansible inventory file like so:
#
# [osds]
# ceph-osd-01 osd_crush_location="{ 'root': 'mon-roottt', 'rack': 'mon-rackkkk', 'pod': 'monpod', 'host': 'ceph-osd-01' }"
#
# Note that 'host' is mandatory and that you need to submit at least two bucket type (including the host)
create_crush_tree: false

##########
# DOCKER #
##########

# Resource limitation
# For the whole list of limits you can apply see: docs.docker.com/engine/admin/resource_constraints
# Default values are based from: https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html/red_hat_ceph_storage_hardware_guide/minimum_recommendations
# These options can be passed using the 'ceph_mon_docker_extra_env' variable.
ceph_mon_docker_memory_limit: "{{ ansible_memtotal_mb }}m"
ceph_mon_docker_cpu_limit: 1
ceph_mon_container_listen_port: 3300

# Use this variable to add extra env configuration to run your mon container.
# If you want to set a custom admin keyring you can set this variable like following:
# ceph_mon_docker_extra_env: -e ADMIN_SECRET={{ admin_secret }}
ceph_mon_docker_extra_env:
mon_docker_privileged: false
mon_docker_net_host: true
ceph_config_keys: [] # DON'T TOUCH ME


###########
# SYSTEMD #
###########
# ceph_mon_systemd_overrides will override the systemd settings
# for the ceph-mon services.
# For example,to set "PrivateDevices=false" you can specify:
#ceph_mon_systemd_overrides:
#  Service:
#    PrivateDevices: False
