[tox]
envlist = {dev,jewel,luminous,mimic,rhcs}-{xenial_cluster,centos7_cluster,docker_cluster,update_cluster,cluster,update_docker_cluster,switch_to_containers,purge_filestore_osds_container,purge_filestore_osds_non_container,purge_cluster_non_container,purge_cluster_container,ooo_collocation}
  {dev,luminous,mimic,rhcs}-{filestore_osds_non_container,filestore_osds_container,bluestore_osds_container,bluestore_osds_non_container,bluestore_lvm_osds,bluestore_lvm_osds_container,lvm_osds,purge_lvm_osds,shrink_mon,shrink_osd,shrink_mon_container,shrink_osd_container,docker_cluster_collocation,purge_bluestore_osds_non_container,purge_bluestore_osds_container,lvm_batch,lvm_osds_container,lvm_batch_container}
  infra_lv_create

skipsdist = True

# a test scenario for the lv-create.yml and lv-teardown playbooks
[testenv:infra_lv_create]
whitelist_externals =
    vagrant
    bash
    cp
    mkdir
    cat
passenv=*
setenv=
  ANSIBLE_SSH_ARGS = -F {changedir}/vagrant_ssh_config
  ANSIBLE_CONFIG = -F {toxinidir}/ansible.cfg
  ANSIBLE_ACTION_PLUGINS = {toxinidir}/plugins/actions
  ANSIBLE_CALLBACK_PLUGINS = {toxinidir}/plugins/callback
  ANSIBLE_CALLBACK_WHITELIST = profile_tasks
  # only available for ansible >= 2.2
  ANSIBLE_STDOUT_CALLBACK = debug
deps= -r{toxinidir}/tests/requirements.txt
changedir={toxinidir}/tests/functional/centos/7/infra_lv_create
commands=
  vagrant up --no-provision {posargs:--provider=virtualbox}
  bash {toxinidir}/tests/scripts/generate_ssh_config.sh {changedir}

  cp {toxinidir}/infrastructure-playbooks/lv-create.yml {toxinidir}/lv-create.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/lv-create.yml

  cp {toxinidir}/infrastructure-playbooks/lv-teardown.yml {toxinidir}/lv-teardown.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/lv-teardown.yml --extra-vars "ireallymeanit=yes"

  cat {toxinidir}/lv-create.log

  vagrant destroy --force

# extra commands for purging clusters
# that purge the cluster and then set it up again to
# ensure that a purge can clear nodes well enough that they
# can be redployed to.
[purge]
commands=
  cp {toxinidir}/infrastructure-playbooks/{env:PURGE_PLAYBOOK:purge-cluster.yml} {toxinidir}/{env:PURGE_PLAYBOOK:purge-cluster.yml}
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/{env:PURGE_PLAYBOOK:purge-cluster.yml} --extra-vars "\
      ireallymeanit=yes \
      remove_packages=yes \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest} \
  "

  # set up the cluster again
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "\
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest} \
      ceph_dev_branch={env:CEPH_DEV_BRANCH:master} \
      ceph_dev_sha1={env:CEPH_DEV_SHA1:latest} \
  "
  # test that the cluster can be redeployed in a healthy state
  testinfra -n 4 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts {toxinidir}/tests/functional/tests

[purge-lvm]
commands=
  cp {toxinidir}/infrastructure-playbooks/{env:PURGE_PLAYBOOK:purge-cluster.yml} {toxinidir}/{env:PURGE_PLAYBOOK:purge-cluster.yml}
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/{env:PURGE_PLAYBOOK:purge-cluster.yml} --extra-vars "\
      ireallymeanit=yes \
      remove_packages=yes \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest} \
  "

  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/lvm_setup.yml

  # set up the cluster again
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "\
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest} \
      ceph_dev_branch={env:CEPH_DEV_BRANCH:master} \
      ceph_dev_sha1={env:CEPH_DEV_SHA1:latest} \
  "
  # test that the cluster can be redeployed in a healthy state
  testinfra -n 4 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts {toxinidir}/tests/functional/tests

# extra commands for performing a rolling update
# currently this hardcodes the release to kraken
# as we're still installing jewel by default
[update]
commands=
  cp {toxinidir}/infrastructure-playbooks/rolling_update.yml {toxinidir}/rolling_update.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/rolling_update.yml --extra-vars "\
      ireallymeanit=yes \
      ceph_stable_release={env:UPDATE_CEPH_STABLE_RELEASE:kraken} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:UPDATE_CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:UPDATE_CEPH_DOCKER_IMAGE_TAG:latest} \
      ceph_dev_branch={env:UPDATE_CEPH_DEV_BRANCH:master} \
      ceph_dev_sha1={env:UPDATE_CEPH_DEV_SHA1:latest} \
  "

  bash -c "CEPH_STABLE_RELEASE={env:UPDATE_CEPH_STABLE_RELEASE:luminous} testinfra -n 4 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts {toxinidir}/tests/functional/tests"

[shrink-mon]
commands=
  cp {toxinidir}/infrastructure-playbooks/shrink-mon.yml {toxinidir}/shrink-mon.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/shrink-mon.yml --extra-vars "\
      ireallymeanit=yes \
      mon_to_kill={env:MON_TO_KILL:ceph-mon2} \
  "
[shrink-osd]
commands=
  cp {toxinidir}/infrastructure-playbooks/shrink-osd.yml {toxinidir}/shrink-osd.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/shrink-osd.yml --extra-vars "\
      ireallymeanit=yes \
      osd_to_kill=0 \
  "

[switch-to-containers]
commands=
  cp {toxinidir}/infrastructure-playbooks/switch-from-non-containerized-to-containerized-ceph-daemons.yml {toxinidir}/switch-from-non-containerized-to-containerized-ceph-daemons.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/switch-from-non-containerized-to-containerized-ceph-daemons.yml --extra-vars "\
      ireallymeanit=yes \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest} \
      ceph_dev_branch={env:CEPH_DEV_BRANCH:master} \
      ceph_dev_sha1={env:CEPH_DEV_SHA1:latest} \
  "

  testinfra -n 4 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts-switch-to-containers {toxinidir}/tests/functional/tests

[testenv]
whitelist_externals =
    vagrant
    bash
    pip
    cp
    sleep
passenv=*
sitepackages=True
setenv=
  ANSIBLE_SSH_ARGS = -F {changedir}/vagrant_ssh_config
  ANSIBLE_CONFIG = -F {toxinidir}/ansible.cfg
  ANSIBLE_ACTION_PLUGINS = {toxinidir}/plugins/actions
  ANSIBLE_CALLBACK_PLUGINS = {toxinidir}/plugins/callback
  ANSIBLE_CALLBACK_WHITELIST = profile_tasks
  # only available for ansible >= 2.2
  ANSIBLE_STDOUT_CALLBACK = debug
  ansible2.2: DELEGATE_FACTS_HOST = False
  docker_cluster: PLAYBOOK = site-docker.yml.sample
  docker_cluster_collocation: PLAYBOOK = site-docker.yml.sample
  ooo_collocation: PLAYBOOK = site-docker.yml.sample
  update_docker_cluster: PLAYBOOK = site-docker.yml.sample
  purge_cluster_container: PLAYBOOK = site-docker.yml.sample
  purge_cluster_container: PURGE_PLAYBOOK = purge-docker-cluster.yml
  purge_bluestore_osds_container: PLAYBOOK = site-docker.yml.sample
  purge_bluestore_osds_container: PURGE_PLAYBOOK = purge-docker-cluster.yml
  purge_filestore_osds_container: PLAYBOOK = site-docker.yml.sample
  purge_filestore_osds_container: PURGE_PLAYBOOK = purge-docker-cluster.yml

  filestore_osds_container: PLAYBOOK = site-docker.yml.sample
  bluestore_osds_container: PLAYBOOK = site-docker.yml.sample
  shrink_mon_container: PLAYBOOK = site-docker.yml.sample
  shrink_mon_container: MON_TO_KILL = mon2
  shrink_osd_container: PLAYBOOK = site-docker.yml.sample
  shrink_osd_container: COPY_ADMIN_KEY = True
  shrink_osd: COPY_ADMIN_KEY = True

  rhcs: CEPH_STABLE_RELEASE = luminous
  jewel: CEPH_STABLE_RELEASE = jewel
  jewel: CEPH_DOCKER_IMAGE_TAG = latest-jewel
  jewel: UPDATE_CEPH_STABLE_RELEASE = luminous
  jewel: UPDATE_CEPH_DOCKER_IMAGE_TAG = latest-luminous
  jewel: CEPH_DOCKER_IMAGE_TAG_BIS = latest-bis-jewel
  luminous: CEPH_STABLE_RELEASE = luminous
  luminous: CEPH_DOCKER_IMAGE_TAG = latest-luminous
  luminous: CEPH_DOCKER_IMAGE_TAG_BIS = latest-bis-luminous
  luminous: UPDATE_CEPH_STABLE_RELEASE = luminous
  luminous: UPDATE_CEPH_DOCKER_IMAGE_TAG = latest
  mimic: CEPH_STABLE_RELEASE = mimic
  mimic: CEPH_DOCKER_IMAGE_TAG = latest-mimic
  mimic: CEPH_DOCKER_IMAGE_TAG_BIS = latest-bis
  mimic: UPDATE_CEPH_STABLE_RELEASE = mimic
  mimic: UPDATE_CEPH_DOCKER_IMAGE_TAG = latest
  lvm_osds: CEPH_STABLE_RELEASE = luminous
  lvm_osds: PLAYBOOK = site.yml.sample
  lvm_osds_container: CEPH_STABLE_RELEASE = luminous
  lvm_osds_container: PLAYBOOK = site-docker.yml.sample
  bluestore_lvm_osds: CEPH_STABLE_RELEASE = luminous
  bluestore_lvm_osds_container: CEPH_STABLE_RELEASE = luminous
  update_cluster: ROLLING_UPDATE = True
  update_docker_cluster: ROLLING_UPDATE = True
deps= -r{toxinidir}/tests/requirements.txt
changedir=
  # tests a 1 mon, 1 osd, 1 mds and 1 rgw xenial cluster using non-collocated OSD scenario
  xenial_cluster: {toxinidir}/tests/functional/ubuntu/16.04/cluster
  # tests a 1 mon, 1 osd, 1 mds and 1 rgw centos7 cluster using non-collocated OSD scenario
  centos7_cluster: {toxinidir}/tests/functional/centos/7/cluster
  filestore_osds_container: {toxinidir}/tests/functional/centos/7/fs-osds-container
  bluestore_osds_container: {toxinidir}/tests/functional/centos/7/bs-osds-container
  filestore_osds_non_container: {toxinidir}/tests/functional/centos/7/fs-osds-non-container
  bluestore_osds_non_container: {toxinidir}/tests/functional/centos/7/bs-osds-non-container
  shrink_mon: {toxinidir}/tests/functional/centos/7/cluster
  shrink_mon_container: {toxinidir}/tests/functional/centos/7/docker
  shrink_osd: {toxinidir}/tests/functional/centos/7/shrink_osd
  shrink_osd_container: {toxinidir}/tests/functional/centos/7/shrink_osd_container
  # an alias for centos7_cluster, this makes the name better suited for rhcs testing
  cluster: {toxinidir}/tests/functional/centos/7/cluster
  # tests a 1 mon, 1 osd, 1 mds and 1 rgw centos7 cluster using docker
  docker_cluster: {toxinidir}/tests/functional/centos/7/docker
  docker_cluster_collocation: {toxinidir}/tests/functional/centos/7/docker-collocation
  update_docker_cluster: {toxinidir}/tests/functional/centos/7/docker
  purge_filestore_osds_container: {toxinidir}/tests/functional/centos/7/fs-osds-container
  purge_bluestore_osds_container: {toxinidir}/tests/functional/centos/7/bs-osds-container
  purge_filestore_osds_non_container: {toxinidir}/tests/functional/centos/7/fs-osds-non-container
  purge_bluestore_osds_non_container: {toxinidir}/tests/functional/centos/7/bs-osds-non-container
  purge_cluster_non_container: {toxinidir}/tests/functional/centos/7/cluster
  purge_cluster_container: {toxinidir}/tests/functional/centos/7/docker
  update_cluster: {toxinidir}/tests/functional/centos/7/cluster
  switch_to_containers: {toxinidir}/tests/functional/centos/7/cluster
  lvm_osds: {toxinidir}/tests/functional/centos/7/lvm-osds
  lvm_osds_container: {toxinidir}/tests/functional/centos/7/lvm-osds-container
  lvm_batch: {toxinidir}/tests/functional/centos/7/lvm-batch
  lvm_batch_container: {toxinidir}/tests/functional/centos/7/lvm-batch-container
  bluestore_lvm_osds: {toxinidir}/tests/functional/centos/7/bs-lvm-osds
  bluestore_lvm_osds_container: {toxinidir}/tests/functional/centos/7/bs-lvm-osds-container
  purge_lvm_osds: {toxinidir}/tests/functional/centos/7/lvm-osds
  ooo_collocation: {toxinidir}/tests/functional/centos/7/ooo-collocation

commands=
  rhcs: ansible-playbook -vv -i "localhost," -c local {toxinidir}/tests/functional/rhcs_setup.yml --extra-vars "change_dir={changedir}" --tags "vagrant_setup"
  dev: ansible-playbook -vv -i "localhost," -c local {toxinidir}/tests/functional/dev_setup.yml --extra-vars "change_dir={changedir} ceph_dev_branch={env:CEPH_DEV_BRANCH:master} ceph_dev_sha1={env:CEPH_DEV_SHA1:latest}" --tags "vagrant_setup"

  vagrant up --no-provision {posargs:--provider=virtualbox}
  bash {toxinidir}/tests/scripts/generate_ssh_config.sh {changedir}

  lvm_osds: ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/lvm_setup.yml
  lvm_osds_container: ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/lvm_setup.yml
  bluestore_lvm_osds: ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/lvm_setup.yml
  purge_lvm_osds: ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/lvm_setup.yml

  rhcs: ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/rhcs_setup.yml --extra-vars "ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} repo_url={env:REPO_URL:} rhel7_repo_url={env:RHEL7_REPO_URL:}" --skip-tags "vagrant_setup"

  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/setup.yml

  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "\
      delegate_facts_host={env:DELEGATE_FACTS_HOST:True} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:mimic} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest} \
      ceph_dev_branch={env:CEPH_DEV_BRANCH:master} \
      ceph_dev_sha1={env:CEPH_DEV_SHA1:latest} \
      copy_admin_key={env:COPY_ADMIN_KEY:False} \
  "

  # wait 30sec for services to be ready
  sleep 30
  # test cluster state using ceph-ansible tests
  testinfra -n 8 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts {toxinidir}/tests/functional/tests

  # reboot all vms
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/reboot.yml

  # wait 30sec for services to be ready
  sleep 30
  # retest to ensure cluster came back up correctly after rebooting
  testinfra -n 8 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts {toxinidir}/tests/functional/tests

  # handlers/idempotency test
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/{env:PLAYBOOK:site.yml.sample} \
      --extra-vars "\
      delegate_facts_host={env:DELEGATE_FACTS_HOST:True} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:mimic} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG_BIS:latest-bis} \
      ceph_dev_branch={env:CEPH_DEV_BRANCH:master} \
      ceph_dev_sha1={env:CEPH_DEV_SHA1:latest} \
      copy_admin_key={env:COPY_ADMIN_KEY:False} " \
      --extra-vars @ceph-override.json

  purge_cluster_non_container: {[purge]commands}
  purge_cluster_container: {[purge]commands}
  purge_filestore_osds_non_container: {[purge]commands}
  purge_bluestore_osds_non_container: {[purge]commands}
  purge_filestore_osds_container: {[purge]commands}
  purge_bluestore_osds_container: {[purge]commands}
  purge_lvm_osds: {[purge-lvm]commands}
  switch_to_containers: {[switch-to-containers]commands}
  update_cluster: {[update]commands}
  update_docker_cluster: {[update]commands}
  shrink_mon: {[shrink-mon]commands}
  shrink_mon_container: {[shrink-mon]commands}
  shrink_osd: {[shrink-osd]commands}
  shrink_osd_container: {[shrink-osd]commands}

  vagrant destroy --force
