[tox]
envlist = {centos,ubuntu}-{container,non_container}-{all_daemons,collocation,lvm_osds,shrink_mon,shrink_osd,shrink_osd_legacy,lvm_batch,add_osds,rgw_multisite,purge,lvm_auto_discovery,all_in_one}
  {centos,ubuntu}-non_container-switch_to_containers
  {centos,ubuntu}-container-{cluster,ooo_collocation,infra_lv_create}
  infra_lv_create

skipsdist = True

# a test scenario for the lv-create.yml and lv-teardown playbooks
[testenv:infra_lv_create]
whitelist_externals =
    vagrant
    bash
    cp
    mkdir
    cat
passenv=*
setenv=
  ANSIBLE_SSH_ARGS = -F {changedir}/vagrant_ssh_config -o ControlMaster=auto -o ControlPersist=600s -o PreferredAuthentications=publickey
  ANSIBLE_CONFIG = {toxinidir}/ansible.cfg
  ANSIBLE_ACTION_PLUGINS = {toxinidir}/plugins/actions
  ANSIBLE_CALLBACK_PLUGINS = {toxinidir}/plugins/callback
  ANSIBLE_CALLBACK_WHITELIST = profile_tasks
  # only available for ansible >= 2.2
  ANSIBLE_STDOUT_CALLBACK = debug
deps= -r{toxinidir}/tests/requirements.txt
changedir={toxinidir}/tests/functional/centos/7/infra_lv_create
commands=
  vagrant up --no-provision {posargs:--provider=virtualbox}
  bash {toxinidir}/tests/scripts/generate_ssh_config.sh {changedir}

  cp {toxinidir}/infrastructure-playbooks/lv-create.yml {toxinidir}/lv-create.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/lv-create.yml

  cp {toxinidir}/infrastructure-playbooks/lv-teardown.yml {toxinidir}/lv-teardown.yml
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/lv-teardown.yml --extra-vars "ireallymeanit=yes"

  cat {toxinidir}/lv-create.log

  vagrant destroy --force

# extra commands for purging clusters
# that purge the cluster and then set it up again to
# ensure that a purge can clear nodes well enough that they
# can be redployed to.
[purge]
commands=
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/tests/functional/rbd_map_devices.yml --extra-vars "\
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
  "

  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/infrastructure-playbooks/{env:PURGE_PLAYBOOK:purge-cluster.yml} --extra-vars "\
      ireallymeanit=yes \
      remove_packages=yes \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
  "

  # re-setup lvm
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/tests/functional/lvm_setup.yml

  # set up the cluster again
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars @ceph-override.json --extra-vars "\
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
  "
  # test that the cluster can be redeployed in a healthy state
  py.test -n 8 --durations=0 --sudo -v --connection=ansible --ansible-inventory={changedir}/{env:INVENTORY} --ssh-config={changedir}/vagrant_ssh_config {toxinidir}/tests/functional/tests

[purge-lvm]
commands=
  cp {toxinidir}/infrastructure-playbooks/{env:PURGE_PLAYBOOK:purge-cluster.yml} {toxinidir}/{env:PURGE_PLAYBOOK:purge-cluster.yml}
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/{env:PURGE_PLAYBOOK:purge-cluster.yml} --extra-vars "\
      ireallymeanit=yes \
      remove_packages=yes \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
  "

  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/tests/functional/lvm_setup.yml

  # set up the cluster again
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "\
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
  "
  # test that the cluster can be redeployed in a healthy state
  py.test -n 8 --durations=0 --sudo -v --connection=ansible --ansible-inventory={changedir}/{env:INVENTORY} --ssh-config={changedir}/vagrant_ssh_config {toxinidir}/tests/functional/tests

[shrink-mon]
commands=
  cp {toxinidir}/infrastructure-playbooks/shrink-mon.yml {toxinidir}/shrink-mon.yml
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/shrink-mon.yml --extra-vars "\
      ireallymeanit=yes \
      mon_to_kill={env:MON_TO_KILL:mon2} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
  "
[shrink-osd]
commands=
  cp {toxinidir}/infrastructure-playbooks/shrink-osd.yml {toxinidir}/shrink-osd.yml
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/shrink-osd.yml --extra-vars "\
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
      ireallymeanit=yes \
      osd_to_kill=0 \
  "

[shrink-osd-legacy]
commands=
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/infrastructure-playbooks/shrink-osd-ceph-disk.yml --extra-vars "\
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
      ireallymeanit=yes \
      osd_to_kill=0,3 \
  "

[switch-to-containers]
commands=
  cp {toxinidir}/infrastructure-playbooks/switch-from-non-containerized-to-containerized-ceph-daemons.yml {toxinidir}/switch-from-non-containerized-to-containerized-ceph-daemons.yml
  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/switch-from-non-containerized-to-containerized-ceph-daemons.yml --extra-vars "\
      ireallymeanit=yes \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
  "

  py.test -n 8 --durations=0 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts-switch-to-containers --ssh-config={changedir}/vagrant_ssh_config {toxinidir}/tests/functional/tests

[add-osds]
commands=
  ansible-playbook -vv -i {changedir}/hosts-2 --limit osd1 {toxinidir}/tests/functional/setup.yml
  ansible-playbook -vv -i {changedir}/hosts-2 --limit osd1 {toxinidir}/tests/functional/lvm_setup.yml
  #cp {toxinidir}/infrastructure-playbooks/add-osd.yml {toxinidir}/add-osd.yml
  #ansible-playbook -vv -i {changedir}/hosts-2 --limit osd1 {toxinidir}/add-osd.yml --extra-vars "\
  ansible-playbook -vv -i {changedir}/hosts-2 --limit osd1 {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "\
      ireallymeanit=yes \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
      "
  py.test -n 8 --durations=0 --sudo -v --connection=ansible --ansible-inventory={changedir}/hosts-2 --ssh-config={changedir}/vagrant_ssh_config {toxinidir}/tests/functional/tests

[rgw-multisite]
commands=
  bash -c "cd {changedir}/secondary && vagrant up --no-provision {posargs:--provider=virtualbox}"
  bash -c "cd {changedir}/secondary && bash {toxinidir}/tests/scripts/generate_ssh_config.sh {changedir}/secondary"
  ansible-playbook --ssh-common-args='-F {changedir}/secondary/vagrant_ssh_config -o ControlMaster=auto -o ControlPersist=600s -o PreferredAuthentications=publickey' -vv -i {changedir}/secondary/hosts {toxinidir}/tests/functional/setup.yml
  ansible-playbook --ssh-common-args='-F {changedir}/secondary/vagrant_ssh_config -o ControlMaster=auto -o ControlPersist=600s -o PreferredAuthentications=publickey' -vv -i {changedir}/secondary/hosts {toxinidir}/tests/functional/lvm_setup.yml
  # ensure the rule isn't already present
  ansible -i localhost, all -c local -b -m iptables -a 'chain=FORWARD protocol=tcp source=192.168.0.0/16 destination=192.168.0.0/16 jump=ACCEPT action=insert rule_num=1 state=absent'
  ansible -i localhost, all -c local -b -m iptables -a 'chain=FORWARD protocol=tcp source=192.168.0.0/16 destination=192.168.0.0/16 jump=ACCEPT action=insert rule_num=1 state=present'
  ansible-playbook --ssh-common-args='-F {changedir}/secondary/vagrant_ssh_config -o ControlMaster=auto -o ControlPersist=600s -o PreferredAuthentications=publickey' -vv -i {changedir}/secondary/hosts {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "\
      ireallymeanit=yes \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/secondary/fetch} \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
      "
  ansible-playbook -vv -i {changedir}/hosts {toxinidir}/tests/functional/rgw_multisite.yml --extra-vars "ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest}"
  ansible-playbook --ssh-common-args='-F {changedir}/secondary/vagrant_ssh_config -o ControlMaster=auto -o ControlPersist=600s -o PreferredAuthentications=publickey' -vv -i {changedir}/secondary/hosts {toxinidir}/tests/functional/rgw_multisite.yml --extra-vars "ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest}"
  bash -c "cd {changedir}/secondary && vagrant destroy --force"
  ansible -i localhost, all -c local -b -m iptables -a 'chain=FORWARD protocol=tcp source=192.168.0.0/16 destination=192.168.0.0/16 jump=ACCEPT action=insert rule_num=1 state=absent'

[testenv]
whitelist_externals =
    vagrant
    bash
    pip
    cp
    sleep
    rm
passenv=*
sitepackages=True
setenv=
  ANSIBLE_SSH_ARGS = -F {changedir}/vagrant_ssh_config -o ControlMaster=auto -o ControlPersist=600s -o PreferredAuthentications=publickey
  ANSIBLE_CONFIG = {toxinidir}/ansible.cfg
  ANSIBLE_ACTION_PLUGINS = {toxinidir}/plugins/actions
  ANSIBLE_CALLBACK_PLUGINS = {toxinidir}/plugins/callback
  ANSIBLE_CALLBACK_WHITELIST = profile_tasks
  ANSIBLE_STDOUT_CALLBACK = debug
  ANSIBLE_KEEP_REMOTE_FILES = 1
  ANSIBLE_CACHE_PLUGIN = memory
  ANSIBLE_GATHERING = implicit
  # only available for ansible >= 2.5
  centos: CEPH_ANSIBLE_VAGRANT_BOX = centos/7
  fedora: CEPH_ANSIBLE_VAGRANT_BOX = fedora/29-atomic-host
  # Set the vagrant box image to use
  centos-non_container: CEPH_ANSIBLE_VAGRANT_BOX = centos/7
  centos-container: CEPH_ANSIBLE_VAGRANT_BOX = centos/atomic-host
  ubuntu: CEPH_ANSIBLE_VAGRANT_BOX = ceph/ubuntu-xenial

  # Set the ansible inventory host file to be used according to which distrib we are running on
  ubuntu: _INVENTORY = hosts-ubuntu
  INVENTORY = {env:_INVENTORY:hosts}
  container: CONTAINER_DIR = /container
  container: PLAYBOOK = site-docker.yml.sample
  container: PURGE_PLAYBOOK = purge-docker-cluster.yml
  non_container: PLAYBOOK = site.yml.sample
  purge: COPY_ADMIN_KEY = True
  shrink_mon: MON_TO_KILL = mon2
  shrink_osd: COPY_ADMIN_KEY = True
  shrink_osd_legacy: COPY_ADMIN_KEY = True
  ooo_collocation: CEPH_DOCKER_IMAGE_TAG = v3.2.5-stable-3.2-luminous-centos-7
  CEPH_STABLE_RELEASE = luminous

deps= -r{toxinidir}/tests/requirements.txt
changedir=
  all_daemons: {toxinidir}/tests/functional/all_daemons{env:CONTAINER_DIR:}
  cluster: {toxinidir}/tests/functional/all_daemons{env:CONTAINER_DIR:}
  shrink_mon: {toxinidir}/tests/functional/shrink_mon{env:CONTAINER_DIR:}
  shrink_osd: {toxinidir}/tests/functional/shrink_osd{env:CONTAINER_DIR:}
  shrink_osd_legacy: {toxinidir}/tests/functional/shrink_osd_legacy{env:CONTAINER_DIR:}
  # tests a 1 mon, 1 osd, 1 mds and 1 rgw centos7 cluster using docker
  collocation: {toxinidir}/tests/functional/collocation{env:CONTAINER_DIR:}
  purge: {toxinidir}/tests/functional/all_daemons{env:CONTAINER_DIR:}
  switch_to_containers: {toxinidir}/tests/functional/all_daemons
  lvm_osds: {toxinidir}/tests/functional/lvm-osds{env:CONTAINER_DIR:}
  lvm_batch: {toxinidir}/tests/functional/lvm-batch{env:CONTAINER_DIR:}
  lvm_auto_discovery: {toxinidir}/tests/functional/lvm-auto-discovery{env:CONTAINER_DIR:}
  ooo_collocation: {toxinidir}/tests/functional/ooo-collocation
  add_osds: {toxinidir}/tests/functional/add-osds{env:CONTAINER_DIR:}
  rgw_multisite: {toxinidir}/tests/functional/rgw-multisite{env:CONTAINER_DIR:}
  all_in_one: {toxinidir}/tests/functional/all-in-one{env:CONTAINER_DIR:}

commands=
  rhcs: ansible-playbook -vv -i "localhost," -c local {toxinidir}/tests/functional/rhcs_setup.yml --extra-vars "change_dir={changedir}" --tags "vagrant_setup"

  bash {toxinidir}/tests/scripts/vagrant_up.sh --no-provision {posargs:--provider=virtualbox}
  bash {toxinidir}/tests/scripts/generate_ssh_config.sh {changedir}

  # configure lvm
  !lvm_batch-!lvm_auto_discovery: ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/tests/functional/lvm_setup.yml

  rhcs: ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/tests/functional/rhcs_setup.yml --extra-vars "ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} repo_url={env:REPO_URL:} rhel7_repo_url={env:RHEL7_REPO_URL:}" --skip-tags "vagrant_setup"

  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/tests/functional/setup.yml

  ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "\
      delegate_facts_host={env:DELEGATE_FACTS_HOST:True} \
      fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} \
      ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} \
      ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} \
      ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} \
      ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG:latest-luminous} \
      copy_admin_key={env:COPY_ADMIN_KEY:False} \
  "

  # wait 30sec for services to be ready
  sleep 30
  # test cluster state using ceph-ansible tests
  py.test -n 8 --durations=0 --sudo -v --connection=ansible --ansible-inventory={changedir}/{env:INVENTORY} --ssh-config={changedir}/vagrant_ssh_config {toxinidir}/tests/functional/tests

  # reboot all vms
  all_daemons: ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/tests/functional/reboot.yml

  # wait 30sec for services to be ready
  # retest to ensure cluster came back up correctly after rebooting
  all_daemons: py.test -n 8 --durations=0 --sudo -v --connection=ansible --ansible-inventory={changedir}/{env:INVENTORY} --ssh-config={changedir}/vagrant_ssh_config {toxinidir}/tests/functional/tests

  # handlers/idempotency test
  all_daemons,all_in_one: ansible-playbook -vv -i {changedir}/{env:INVENTORY} {toxinidir}/{env:PLAYBOOK:site.yml.sample} --extra-vars "delegate_facts_host={env:DELEGATE_FACTS_HOST:True} fetch_directory={env:FETCH_DIRECTORY:{changedir}/fetch} ceph_stable_release={env:CEPH_STABLE_RELEASE:luminous} ceph_docker_registry={env:CEPH_DOCKER_REGISTRY:docker.io} ceph_docker_image={env:CEPH_DOCKER_IMAGE:ceph/daemon} ceph_docker_image_tag={env:CEPH_DOCKER_IMAGE_TAG_BIS:latest-bis-luminous} copy_admin_key={env:COPY_ADMIN_KEY:False} " --extra-vars @ceph-override.json

  purge: {[purge]commands}
  switch_to_containers: {[switch-to-containers]commands}
  shrink_mon: {[shrink-mon]commands}
  shrink_osd: {[shrink-osd]commands}
  shrink_osd_legacy: {[shrink-osd-legacy]commands}
  add_osds: {[add-osds]commands}
  rgw_multisite: {[rgw-multisite]commands}

  vagrant destroy --force
